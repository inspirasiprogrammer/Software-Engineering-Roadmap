![Markdown Logo](assets/images/algorithms-and-data-structres.jpg)
<small>Visual: Unsplash / Lorenzo Herrera</small>
<br/><br/>

<div align="center">

# **Algorithms and data structres üß±**
 Learn and master Algorithms, Data structres, Asymptotic analysis, Recursion, Dynamic Programming, Divide and conquer and all kina awesomeness üßÉ

<br>


[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://GitHub.com/Naereen/StrapDown.js/graphs/commit-activity) [![Ask Me Anything !](https://img.shields.io/badge/Ask%20me-anything-1abc9c.svg)](https://github.com/humamaboalraja) [![MIT license](https://img.shields.io/badge/License-MIT-blue.svg)](https://lbesson.mit-license.org/)


</div>

---

<br/>

## - [**Table of content**](#table-of-content)

  - 1 . [Getting started](#getting-started)
  - 2 . [Learning Checklist ‚úÖ](#learning-checklist-)
  - ## 3 . [Algorithms Applications & theory](#algorithms-applications-&-theory)
    - 1 . [Characteristics of an Algorithm](#characteristics-of-an-algorithm)
    - 2 . [Asymptotic Analysis](#asymptotic-analysis)
      - 1 . [Time Complexity](#time-complexity)
      - 2 . [Space Complexity](#space-complexity)
      - 3 . [Memory](#memory)
    - 3 . [Asymptotic notations](#asymptotic-notations)
      - 1 . [Big-O notation (O)](#big-O-notation-O)
      - 2 . [Omega notation (Œ©)](#omega-notation-œâ)
      - 3 . [Theta notation (Œò)](#theta-notation-Œ∏)
      - 4 . [Differences between Big-O, Big-Œ© and Big-Œò](#differences-between-big-O,-big-Œ©-and-big-Œò)
      - 5 . [Best, Worst, Average Case](#best,-worst,-average-case)
        - [Adding Functions](#adding-functions)
        - [Multiplying Functions](#multiplying-functions)
        - [Properties of Logarithm](#properties-of-logarithm)
        - [Dominance Pecking Order](#dominance-pecking-order)
        - [Same Time Complexity](#same-time-complexity)
    - 4 . [Master Theorem](#algorithm-design-strategies--techniques)
    - 5 . [Algorithm Design Techniques & Strategies](#master-theorem)
    - 6 . [Searching Algorithms](#searching-algorithms)
      - 1 . [Linear Search](#linear-search)
      - 2 . [Binary Search](#binary-search)
      - 3 . [Jump Search](#jump-search)
      - 4 . [A* search algorithm](#a*-search-algorithm)
      - 5 . [Ternary search](#ternary-search)
      - 6 . [Exponential search](#exponential-search)
    - 7 . [Recursion (Recursive Algorithms)](#recursion-recursive-algorithms)
      - 1 . [Differences between Recursion and Iteration](#differences-between-recursion-and-iteration)
      - 2 . [Base case](#base-case) exit condition (when do you stop recursing)
      - 3 . [General (recursive) case](#general-recursive-case) (Recurse)
    - 8 . [Sorting Algorithms](#sorting-algorithms)
      - 1 . [Bubble Sort](#bubble-sort)  
      - 2 . [Selection Sort](#selection-sort)  
      - 3 . [Insertion Sort](#insertion-sort)  
      - 4 . [Merge Sort](#merge-sort)  
      - 5 . [Quick Sort](#quick-sort)  
      - 6 . [Counting Sort](#counting-sort)  
      - 7 . [Radix Sort](#radix-sort)  
      - 8 . [Bucket Sort](#bucket-sort)  
    - 9 . [Graph Algorithms](#graph-algorithms)
      - 1 . [Breadth-first search](#breadth-first-search)
      - 2 . [Depth-first search](#depth-first-search)
      - 3 . [Dijkstra‚Äôs shortest path algorithm]()
      - 4 . [Bellman‚ÄìFord algorithm](#bellman‚Äìford-algorithm)
    - 10 . [Greedy Algorithms](#greedy-algorithms)
    - 11 . [Divide and Conquer](#divide-and-conquer)
    - 12 . [Dynamic Programming](#dynamic-programming)
    - 13 . [Backtracking](#backtracking)
  - ## 4 . [Data structres](#data-structres)
    - 1  . [What are Data structres?](#what-are-data-structres?)
    - 2  . [Logarithm](#logarithm)
    - 3  . [Difference between Linear and Non-linear Data Structures](#difference-between-linear-and-non-linear-data-structures)
    - 4  . [Arrays](#arrays)
    - 5  . [Linked lists](#linked-lists)
      - 5.1  . [Singly Linked lists](#singly-linked-lists)
      - 5.2  . [Doubly Linked lists](#doubly-linked-lists)
      - 5.2  . [Circular Linked lists](#circular-linked-lists)
    - 6  . [Hash tables](#hash-tables)
    - 7  . [Stacks](#stacks)
    - 8 . [Queues](#queues)
    - 9 . [Trees](#trees)
      - 9.1 . [Binary search trees](#binary-search-trees)
      - 9.2 . [AVL Trees](#avl-trees)
      - 9.3 . [Heaps](#heaps)
    - 10 . [Tries](#tries)
    - 11 . [Graphs](#graphs)
    - 12 . [Strings](#strings)
    - 13 . [Priority Queue](#priority-queue)
    - 14 . [Dictionaries](#dictionaries)
    - 15 . [Matrix](#Matrix)
    
    <br>

    #### **Further Learning Resource**

  - 5 . [Articles üì∞](#-articles-)
  - 6 . [Books üìö](#-books-)
  - 7 . [Courses üíª](#-courses-)

---

<br/>

# 1

## **Getting started**

<details open>
  <summary>Let's first get to know why do we need this thing ü•∏ | <b>Click to expand</b></summary>
</br>

  <div align="center">

  ![](https://media.giphy.com/media/3o7btPCcdNniyf0ArS/giphy.gif)
  
  <br>

  ## **What are Data structres and Algorithms?**
  
  </div>

  A computer program is a collection of instructions to perform a specific task. For this, a computer program may need to store data, retrieve data, and perform computations on the data.

  A data structure is a named location that can be used to store and organize data. And, an algorithm is a collection of steps to solve a particular problem. Learning data structures and algorithms allow us to write efficient and optimized computer programs. [Source](https://www.programiz.com/dsa)

  and the differences between them are that a Data Structure is about organising and managing data effectively such that we can perform specific operation efficiently, while an Algorithm is a step-by-step procedure to be followed to reach the desired output. ... Steps in an algorithm can use one or many data structure(s) to solve a problem.



  ---

  Algorithms and Data structures go through solutions to standard problems in detail and give you an insight into how efficient it is to use each one of them, and help in understanding the nature of the problem at a deeper level and that's why it's very crucial to master them as a Software Engineer.

  and eventually:
  > it's all about data, and the most efficent way to play with this data üòâ

  <div align="center">
  Pumped enough! let's get started!
  </div>



---


</details>
<br/>
<br/>
<br/>







# 2
## **Learning Checklist ‚úÖ**

<details>
  <summary>A handy checklist to keep track of your progress, and know when you master your Algorithms and data strucres path üíà. <b>Click to expand</b></summary>
</br>

### **Algorithms**:
Show that you know:

- [ ]  Know how to analyze algorithms
- [ ] A detailed description of the algorithm
    - [ ] Goal of the algorithm
    - [ ] Termination condition
    - [ ] Criteria for determining that the algorithm is correct
    - [ ] Explanation of the steps of the algorithm
- [ ] A specific example illustrating how the algorithm works
- [ ] Detailed calculation of the time complexities, including best, worst and average cases
- [ ] Pseudo-code for complex algorithms (this excludes simple searching and sorting algorithms)
- [ ]  Applied these concepts to searching and sorting algorithms

### **Data structures**:
For each of the data structures listed below, you have to understand their functionality, including common operations and their time complexities; what are their strengths and their limitations and, finally, how they are used in real-world scenarios.

When describing a data structure make sure your analysis includes the following points

- [ ] How does the data structure organize and manipulate data?
- [ ] What operations can you do on it and what are their time complexities and why?
- [ ] What operations can you do on it and what are their time complexities and why?
- [ ] Applications
  
  ---

- [ ]  Arrays
  - [ ] Traversing, Searching, Insertion, Deletion, Size
- [ ]  Stacks
  - [ ]  Push, Pop, is empty, top
- [ ]  Queues
- [ ]  Linked Lists
- [ ]  Hash tables
- [ ]  Graphs
- [ ]  Trees
- [ ]  Tries



</details>

---

<br/>
<br/>





# 3

## **Algorithms Applications & theory**


<br>

<details>

<summary>Algorithms Applications & theory | <b>Click to expand</b></summary>
<br>

<div align="center">

![](https://media.giphy.com/media/6wa5vuYvetU1Jibm13/source.gif)

</div>

---

 <h2>Algorithm</h2> 

noun, UK  /Àà√¶l.…°…ô.r…™.√∞…ôm/ US  /Àà√¶l.…°…ô.r…™.√∞…ôm/

>a process, step-by-step procedure or set of rules to be followed in calculations or other problem-solving operations to be executed in a certain order to get the desired output, especially by a computer, and those can be simple processes, such as multiplying two numbers, or a complex operation, such as playing  compressed video file. or a Search engine that uses proprietary algorithms to display the most relevant results from its search index for specific queries. Algorithms are generally created independent of underlying languages, i.e. an algorithm can be implemented in more than one programming language.


**<small>Cambridge dictionary/ Oxford Languages</small>**

<br>
<details>
<summary>Algorithms Applications & theory | <b>Click to expand üî•</b></summary>

<br>

- **What:** Algorithms are a part of daily life actions, and those daily actions and everything we do is the simplest form to represent what an Algorithm is, e.g Finding you car in a parking lot , cleaning your Apartment, reading a book.
  
  ---

- **How to:** There are no well-defined standards for writing algorithms. Rather, it is problem and resource dependent. Algorithms are never written to support a particular programming code. As we know that all programming languages share basic code constructs like loops (do, for, while), flow-control (if-else), etc. These common constructs can be used to write an algorithm. We write algorithms in a step-by-step manner, but it is not always the case. Algorithm writing is a process and is executed after the problem domain is well-defined. That is, we should know the problem domain, for which we are designing a solution.
  
  ---



- **Applications**: The real power of Algorithms come in form of Digital tools, Softwares or small computer programs like, Compression algorithms in a 3D video game or Searching algorithms in Google Search engine, or sorting algorithms to sort Amazon's products based on their ratings and all the other services and digital tools that you use on daily bases.

___

- **Efficiency**: Not all algorithms are created equal, and the tricky part of an Algorithm is that there are plenty of algorithms that solve the same problem at the end, but one of them is the most efficint one to use in that spesific problem case, so to know which solution to choose and to be able to compare them, these Algorithms most be analyzed, and before before analyuzing there an important thing that you need to know what makes a good algorithm is the two most important criteria which are that it solves a problem and it does so efficintly.

    ---
- **Measuring Efficiency**: so the way we measure the effecincy of an algorithm is through using a scientific mathematical technique called **```Asymptotic analysis```**, which allows algorithms to be compared independently of a particular programming language or hardware, which will next tell us that some algorithms are  more efficient than others.

  ---
  
  <br>

  Some important categories of algorithms, from data structures point of view.

  |# | Details|
  |---|------|
  **Search** | Algorithm to search an item in a data structure.
  **Sort** | Algorithm to sort items in a certain order.
  **Insert** | Algorithm to insert item in a data structure.
  **Update** | Algorithm to update an existing item in a data structure.
  **Delete** | Algorithm to delete an existing item from a data structure.


</details>

</details>

---

<br/>
<br/>







# 3.1
## **Characteristics of an Algorithm**:

</br>

<details>
<summary>Characteristics of an Algorithm | <b>Click to expand</b></summary>

<br>

Not all procedures can be called an algorithm. An algorithm should have the following characteristics.


|No.     |  Details|
|------|-------|
**Unambiguous** | Algorithm should be clear and unambiguous.| Each of its steps (or phases), and their inputs/outputs should be clear and must lead to only one meaning.|
**Input** | An algorithm should have 0 or more well-defined inputs.|
**Output** | An algorithm should have 1 or more well-defined outputs, and should match the desired output.|
**Finiteness** | Algorithms must terminate after a finite number of steps.|
**Feasibility** | Should be feasible with the available resources
**Independent** | An algorithm should have step-by-step directions, which should be independent of any programming code.|
**Correctness** | Every step of the algorithm must generate a correct output.|


<br>



<br>


</details>

---

<br/>
<br/>



# 3.2
## **Asymptotic Analysis**:

</br>


**Asymptotic analysis** of an algorithm refers to defining the mathematical boundation/framing of its run-time performance. 

<details>

<summary>Asymptotic Analysis explanation | <b>Click to expand</b></summary>

<br>

- Using asymptotic analysis, we can very well conclude the best case, average case, and worst case scenario of an algorithm. 


- Complexity analysis is effectively used to determine how "good, efficent, scalable, fits the design case best" an algorithm is and whether it's "better" than another one.

- determining how efficient an algorithm is usually involves finding both the **time** complexity and **space** complexity of an Algorithm.

  ---

  <br>
  <div align="center">

  ![](https://media.giphy.com/media/3o6Yg4GUVgIUg3bf7W/giphy.gif)

  ## **But before that what Asymptotic Analysis is about?**
  
  </div>
  <br>

- First what what asymptotic does really mean?! i'm guessing that you have probably heard of the word "asymptote" if not An asymptote is a "line that continually approaches a given curve but does not meet it at any finite distance, in a simpler way it is line that a curve approaches, as it heads towards infinity like that:

  <div align="center">

  ![](assets/images/asymptotic_notation/asymptotic_analysis/asymptote.png)
  </div>
  <br>
  <br>

- The idea of Asymptotic complexity and looking at an asymptotic behavior is that we want to see how does the graph behave getting into very large inputs (n) values, and this is simply the idea of asymptotic complexity. 
  
- So why don‚Äôt we in this case to save some time measure the **elapsed real time**, like for instance measuring how fast your code ran on your machine which can tell you how strong your algorithm is!? Because it's not the effecint approach to approach Complexity analysis.

- in computer science problems are often applied at a grand scale like for instance if we are writing an algorithm to optimize whatever part of **Google‚Äôs** search engine it‚Äôs going to be used across billions of users, and there will be large inputs to the algorithms. 
  
- Our most important point is to see how does this algorithm behave on the tail end ( as (input) gets very large), cause we can only see the true measure of performance of an algorithm when we have very large data input, and that why asymptotic complexity analysis intrigues us ‚ù§Ô∏è 

- So in order to be able to indicate the correct Asymptotic Analysis of an algorithm there are some rules that you need to follow: 

  1. We measure as a function of $n$, and ignore low order terms, and that's why we drop constants, which is the reason too why:

     - 5n<sup>3</sup> + n  ‚àí 6 becomes n<sup>3</sup>

     - 8n log n  ‚àí <math>60n</math> becomes n log n
     - 2<sup>n</sup> + 3n<sup>4</sup>  ‚àí becomes 2<sup>n</sup>
      - because when we say (O(log n), o(n), O(n2),  O(n3), O(2n), o(n!)) we are not actually describing an individual graph, or a case that is based on a constant value, what we are really describing is a class of functions and behaviors, and that‚Äôs why these functions will have the same behaviour when we get a very large input üòâ
  2. If the function is a product of several factors, any constants can be omitted.




  <br>

## **An example of Asymptotic Behaviour**
  
---

  ```Insertion Sort:``` 2 * n<sup>2</sup>  | ```Merge Sort:``` 50 * n * log(n)
    
  ---

  We have 2 computers: üñ•

  - **Computer A**: runs 10 Billion instructions / second

  - **Computer B**:  runs 10 Million instructions / second

  - **Computer A** is 1000x faster than **Computer B**

  - **Computer A** runs insertion sort, **Computer B** runs merge sort

  - How long will each computer take to sort 10 million numbers?

  - **Computer A**: 5.5 hours
  - **Computer B**: 20 minutes

  A computer that runs **1000x** faster lost horrendously to a computer that runs **1000x** slower than it.

  But the thing is that insertion sort will be faster for an initial amount, but it will lose as the input gets larger (and that's what we care about and what is a true expression of its efficiency).

  ---


</details>
<br/>

</details>

### **Time complexity**:
<details>
  <summary>What time complexity is? | <b>Click to expand</b></summary>
</br>

Time complexity of an algorithm represents the amount of time required by the algorithm to run to completion. Time requirements can be defined as a numerical function T(n), where T(n) can be measured as the number of steps, provided each step consumes constant time.

  - For example, addition of two n-bit integers takes n steps. Consequently, the total computational time is T(n) = c ‚àó n, where c is the time taken for the addition of two bits. Here, we observe that T(n) grows linearly as the input size increases.

- The time complexity of an algorithm relates the length of an algorithm's input to the number of steps it takes.

<br>

---

We do not want to give the running time of an algorithm in a time unit because this would mean that it is only comparable
with the same implementations (programming language, compiler, hardware, etc.). Thus, it is given as a function:




### **The RAM (random access machine) Model of Computation**:

Is a Machine-indepdendent algorithm design  depends upon a hypothetical computer called (Random Access Machine) and under this model of computation we are confronted with a computer where:

- Simple logical or arithmetic operations (+, *, =, if, call) are considered to be simple operations that that take one time step
-  Loops and subroutines are complex operations composed of multiple time steps based on the number of iterations
- All memory access takes exactly one time step.

> This model encapsulates the core functionality of computers but does not mimic them completely. For example, an addition operation and a multiplication operation are both worth a single time step, however, in reality it will take a machine more operations to compute a product versus a sum.

- The reason the RAM model makes these assumptions is because doing so allows a balance between simplicity and completely imitating underlying machine, resulting in a tool that is useful in practice.

- The exact analysis of algorithms is a difficult task. It is the nature of algorithm analysis to be both machine and language independent. For example, if your computer becomes twice as fast after a recent update, the complexity of your algorithm still remains the same.



</details>

</br></br>


### **Space complexity and Auxiliary Space**:
<details >
  <summary>What time complexity is? | <b>Click to expand</b></summary>
</br>

**Space complexity**: of an algorithm represents the total amount of memory space required by the algorithm in its life cycle (including the space of input values) and relates to the length of an algorithm's input to the number of storage locations it uses. The space required by an algorithm is equal to the sum of the following two components:

- A fixed part that is a space required to store certain data and variables, that are independent of the size of the problem. For example, simple variables and constants used, program size, etc.

- A variable part is a space required by variables, whose size depends on the size of the problem. For example, dynamic memory allocation, recursion stack space, etc.



It is also expressed asymptotically. Space complexity is a measure of the amount of working storage an algorithm needs.
This amount is, again, dependent on the input size. 

to find space-complexity, it is enough to calculate the space occupied by the variables used in an algorithm/program.



---

**Auxiliary space**: is not the same as space complexity even though they are sometimes (wrongly) used for the same thing. Space complexity is the sum of the auxiliary space and the input
space, **Auxiliary space** refers to the temporary space required by an algorithm to be used allocated by the algorithm during it's execution to solve the problem, with respect to input size. Think of temporary arrays, pointers etc.

```
Space Complexity = Auxiliary Space + Space use by input values
```

Whenever a solution to a problem is written some memory is required to complete. For any algorithm memory may be used for the following:

  1. Variables (This include the constant values, temporary values)
   1. Program Instruction
   2. Execution

  ---

  Example #1:

  We can use the RAM model to determine the number of steps it will take an algorithm to end with an input we choose, but estimating the worst, average, and best case runtime scenerio with the RAM model can be unconvenint.



  ```python
  def even_numbers_avg(array):
    '''
    This function returns either the average of the 
    sum of even numbers, or None.
    '''
    even_sum = 0           #1 time step
    even_count = 0         #1 time step
                  #n times:
    for n in array:           #1 time step
        if n % 2 == 0:        #1 time step
            even_sum += n         #1 time step
            even_count +=1        #1 time step
            
    if even_count > 0:              #1 time step
        return even_sum/even_count     #1 time step
    else:                           #1 time step
        return None                    #1 time step
  ```
  - In the example above, we can try to generalize the time complexity of this algorithm by counting every step, and we will find that in the worst case its time complexity will be:  
  T(n) = 5n + 6
  - Its space complexity will be the size of the array n, plus the two variables we initialize.
  - The problem with the notation we used above is that is difficult to work precisely with it. In the example above we can see that both return statements have been counted as well as every step in the for loop, which is correct for the worst but not for the average and best case scenarios.
  - Since we can approximate an algorithm to a mathematical function, we can also determine its growth as a function of the input and define an upper bound function (Big O), a lower bound function (Big Œ©), or both (Big Œò) in order to understand how it grows.
  - In Asymptotic Notation, we only consider the fastest growing term without any multiplicative constant. E.g.: in the example above 
   
    **T(n) = 5n + 6 is O(n)** and not **O(5n)**
  - Constant terms (e.g. returning or printing some value) are ignored
    - (constant terms can make a difference if the compared algorithms have the same running time) an that's why In Asymptotic Notation, we only consider the fastest growing term without any multiplicative constant. E.g.: in the example above **T(n) = 5n + 6** is **O(n)** and not **O(5n)**

  - Space complexity
    - the array takes n units of space, as the length can vary
    - the item variable is constant as the new item will be saved and the old item will be discarded
    - thus, we have a space complexity of Œò(ùëõ)
    - if we talk about auxiliary space, the input array would not be counted
      - thus, we have a auxiliary space complexity of Œò(1)

> And this is why the term space complexity cannot be used for auxiliary space complexity.

  ---
<br>

  Example #2: In recursive calls stack space also counts. 
  <br>

   ```python
      def add(number):
          if number <= 0:
              return 0
          return number + add(number-1)
   ```

  Here each call add a level to the stack :
```
  1.  add(4)
  2.    -> add(3)
  3.      -> add(2)
  4.        -> add(1)
  5.          -> add(0)
```

Each of these calls is added to call stack and takes up actual memory.
      
So it takes ```O(n) space```.

However just because you have n calls total doesn‚Äôt mean it takes O(n) space.



---

</details>


</br></br>


### **Memory**:
<details>
  <summary>What you need to know about memory? | <b>Click to expand</b></summary>
</br>
</details>

</br>

---

</details>



<br/>
<br/>

</details>


# 3.2

## **Asymptotic Notations**:

**Recap:** as we've mentioned in ther Asymptotic analysis section The **efficiency** of an algorithm depends on the amount of time, storage and other resources required to execute the algorithm, and an algorithm may not have the same performance for different types of inputs. With the increase in the input size, the performance will change, now this efficiency is measured with the help of Asymptotic Notations üî•

---

<br>

<details>
<summary>Asymptotic mathematical Notations <b> | Click to expand</b></summary>

<br>

**Asymptotic Notations** are Mathematical notations that are used to describe the running time of an algorithm when the input tends towards a particular value or a limiting value.

### for example: 

1. In Bubble sort algorithm which we will get to it later, when the input array is already sorted, the time taken by the algorithm is linear i.e. the best case.

2. But, when the input array is in reverse condition, the algorithm takes the maximum time (**quadratic**) to sort the elements i.e. the worst case.

3. When the input array is neither sorted nor in reverse order, then it takes average time. These durations are denoted using asymptotic notations.

  ---

  ### **Order of Dominance in the Asymptotic Limit**

  some common asymptotic growths, valid for both space and time complexity:

 - Constant: O(1)
 - Logarithmic: O(log(n))
 - Linear: O(n)
 - Quasilinear: O(n‚ãÖlog(n))
 - Quadratic: O(n2)
 - Exponential: O(2n)
 - Factorial: O(n!)

To understand the difference between some of the most common time complexities, take a look at thi graph below, were the x-axys represents the size of the input of the functions and the y-axys represents the result of the functions.

![](assets/images/asymptotic_notation/big_o/big-o-notation.png)

<div align="center">
<b>n! >> 2n >> n2 >> n‚ãÖlogn >> n >> log(n) >> 1</b>
</div>

It is also clear that an efficient algorithm can really make the difference in terms of time and space efficiency, especially as the input size grows.



  ---
<br>

### There are mainly three asymptotic notations which are üßÉ:
<br>

### **Big-O notation (ùëÇ)**: ü§ï (Asymptotic Upper bound)

<details>
  <summary>Big-O notation (O) with code examples | <b>Click to expand</b></summary>

 <br>


 **Big-O**: notation is the formal way to represent the upper bound of the running time of an algorithm. Thus, It measures the worst case time complexity or the longest amount of time an algorithm can possibly take to complete.

 -  Big O notation is usually understood to describe the **worst-case**, complexity of an algorithm, even though the worst-case complexity might differ from the **average-case** complexity.
 - Variables used in Big O notation denote the sizes of inputs to algorithms. For example, **O(n)**  might be the time complexity of an algorithm that traverses through an array of length **n**; similarly, O(n + m) might be the time complexity of an algorithm that traverses through an array of length **n**  and through a string of length **m**
 
 -  e.g. some sorting algorithms have different time complexities
depending on the layout of elements in their input array. In rare cases, their
time complexity will be much worse than in more common cases. Similarly, an
algorithm that takes in a string and performs special operations on uppercase
characters might have a different time complexity when run on an input string
of only uppercase characters vs. on an input string with just a few uppercase
characters.
- when describing the time complexity of an algorithm, it's helpful somtimers to specify what the time complexity refers to. the average case or to the worst case (e.g., "this algorithm runs in O(nlog(n)) time on average and in $O(n^2)$ time in the worse case").

<br>

  <div align="center">

  ![progra](assets/images/asymptotic_notation/big-o.png)

  <small>Big O.
  | Image source / <a href="https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/big-o-notation">Khan Academy</a></small>
<br>
<br>

</div>
The Big O notation is useful when we only have upper bound on time complexity of an algorithm. Many times we easily find an upper bound by simply looking at the algorithm.  
For a function g(n), O(g(n)) is given by the relation:

``` math
O(f(n)) = { 
  g(n): 
  there exist positive constants c >  0 and n0
  such that 0 ‚â§ f(n) ‚â§ c.g(n) for all n ‚â• n0 
}

```

Since Big-o gives the worst-case running time of an algorithm, it is widely used to analyze an algorithm as we are always interested in the worst-case scenario.


Summary:
>The Big O of a function is its asymptotic upper bound. This means that the running time of a function T will be always shorter than that of f . To generalize we can say that a funciton **T(n)** is **O(f(n))** if there is a constant k such that **T(n) < k** ‚ãÖ **f(n)** for large enough **n**.



  
    
  </br>
    

  1. ### **Big O cheatsheet**


      <details>
      <summary>Big-O Complexity table ‚ú® | <b>Click to expand</b></summary>
      </br>




      The following are examples of common complexities and their Big O notations,ordered from fastest to slowest:

      Big O Notation	| Name | Example(s) | Efficiency | Code example|
      |----------------|------|-----------| -------| ----|
      O(1) | Constant | 	Odd or Even number, <br> Look-up table (on average) | üü© | Python, Javascript
      O(log(n)) | Logarithmic | Finding element on sorted array with binary search | üü© | Python, Javascript
      O(n) | Linear | Find max element in unsorted array. <br> Duplicate elements in array with Hash Map | üü© | Python, Javascript
      O(nlog(n)) | Linearithmic | Python, Javascriptorting elements in array with merge sort | üü© | Python, Javascript
      O(n<sup>2</sup>) | Quadratic | # Duplicate elements in array **(na√Øve)**, <br> Sorting array with bubble sort | üü® | Python, Javascript
      O(n<sup>3</sup>) | Cubic | 3 variables equation solver | üü® | Python, Javascript
      O(2<sup>n</sup>) | Exponential | Find all subsets | üü• | Python, Javascript
      O(n!) | Factorial | Find all permutations of a given set/string | üü• | Python, Javascript
      </details>

      ---
  <br>

</details>

  ---
  <br>

  ### **Omega notation (Œ©)**: üòå (Asymptotic Lower bound)

  <details>
    <summary>What is Omega notation (Œ©) | <b>Click to expand</b></summary>
    </br>
  </details>

  ---

  <br>

  ### **Theta notation (Œò)**: üíà (Asymptotic Tight (exact) bound))
  <details>
    <summary>What Asymptotic Analysis is? | <b>Click to expand</b></summary>
    </br>
  </details>
  
  ---
<br/>



<br/>


  ## **Differences between Big-O, Big-Œ© and Big-Œò**

<details>

<summary>a table that explains the diffrerences between most common asymptotic notations</summary>

  <br>

  Big Oh | Big Omega | Big Theta 
  --------|-------------|--------|
  (It is like <=)  rate of growth of an algorithm is less than or equal to a specific value. | It is (like >=) rate of growth is greater than or equal to a specified value | (It is like ==) meaning the rate of growth is equal to a specified value.
  The upper bound of algorithm is represented by Big O notation. Only the above function is bounded by Big O. asymptotic upper bond is it given by Big O notation.| The algorithm‚Äôs lower bound is represented by Omega notation. The asymptotic lower bond is given by Omega notation. | The bounding of function from above and below is represented by theta notation. The exact asymptotic behavior is done by this theta notation.
  Big oh (O) ‚Äì Worst case	| Big Omega (Œ©) ‚Äì Best case | Big Theta (Œò) ‚Äì Average case
  Big-O is a measure of the longest amount of time it could possibly take for the algorithm to complete. | Big- Œ© is take a small amount of time as compare to Big-O it could possibly take for the algorithm to complete. | Big- Œò is take very short amount of time as compare to Big-O and Big-? it could possibly take for the algorithm to complete.
  Mathematically ‚Äì Big Oh is ```0 <=f(n) <= c g(n) for all n>=n0```	| Mathematically ‚Äì Big Omega is ```O<= C g(n) <= f(n) for all n>=n 0```	| Mathematically ‚Äì Big Theta is ```O<=C 2 g(n)<=f(n)<=C 1 g(n) for n>=n 0```

</details>


</details>

<br/>
<br/>



<br/>





# 3.3

## **Algorithm design strategies & Techniques**:

<details>
  <summary>What Asymptotic Analysis is? | <b>Click to expand</b></summary>
</br>

</details>
<br/>

---

<br/>
<br/>
<br/>
<br/>




# 3.4

## **Algorithm design strategies & Techniques**:

<details>
  <summary>What Asymptotic Analysis is? | <b>Click to expand</b></summary>
</br>

</details>
<br/>

---

<br/>
<br/>
<br/>
<br/>



# 3.5

## **Searching Algorithms**:

<details>
<summary>Searching Algorithms explanation & examples</summary>

<div align="center">



</div>

  <br>
  
  ### **Linear Search**: 

  <details >
    <summary>What Linear Search with examples | <b>Click to expand</b></summary>
    </br>

  <div align="center">

  ![](assets/images/algorithms/linear-search.gif)

  <small>Linear Search/ [Programming Simplified](programmingsimplified.com)</small>
  
  </div>

  **Linear search** is a very simple sequential search algorithm search that takes in input an array and a value to search and iterates through the array to search for the value. It usually returns the index of the element (if it is in the array) or ‚àí1 (if the element is not in the array).
  


``` 
 ü¶∂üèΩ Steps:
```

<details>
<summary>Table of steps</summary>

<br>

  1.  Start from the leftmost element of array[ ], and one by one compare target with each element of array[ ] 

  2. If target matches with an element, return the index.

  3. If target doesn‚Äôt match with any of elements,return -1.




</details>
<br>
<br>


``` 
 üêæ Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>

  Linear Search ( Array A, Value x)


   - 1: Set i to 1
   - 2: if i > n then go to step 7
   - 3: if A[i] = x then go to step 6
   - 4: Set i to i + 1
   - 5: Go to Step 2 |The linear fashion|
   - 6: Print Element x Found at index i and go to step 8
   - 7: Print element not found
   - 8: Exit




</details>


<br>
<br>

``` 
 üìü input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>

  Sample input/ Output:
    
``` python
array[] = {10, 20, 80, 30, 60, 50, 110, 100, 130, 170}
target = 110;
```

  Output : 6 | Element target is present at index 6

  ---

``` python
array[] = {10, 20, 80, 30, 60, 50, 110, 100, 130, 170}
target = 175;    
```
  Output : -1 | Element target is not present in array[]


</details>
<br>
<br>


```
 üíª Implementation:
```


<details>
<summary>Python üêç</summary>

<br>

     

  ```python
    def linearSearch(array, target):
        # Linearly search target in array[] 
        for index in range(len(array)): # o(n) linear time
            # If target is present, which is our input for the function
            if array[index] == target:
                # then return its location 
                return index

        return -1

    listOfItemsToSearchIn = [2,9,35,16,2,7,8,22,35,46,57,68,34,213,4,13] # Size N = 15

    matchedIndex = linearSearch(listOfItemsToSearchIn, 13)

    print(matchedIndex)

    # Result: 15

  ```




</details>
<br>
<br>


```
 ‚è≥ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>


<details>
<summary>Time Complexity Analysis</summary>
<br>


- Looking at the linear search algorithm, we assume that this code would need the sum of one iteration times the length of the array plus the time it takes to set up the for loop and returning a value. 
  
  - (**c1 * n + c2**) Where **c1** is the time for one loop iteration, **n** the array length and **c2** the time of the overhead.  We now can say that the running time grows with **n** 
  
  - which means that this algorithm will search for an element by iterating through the whole array of **n**, so in case if the element is not present or positioned as the last element in the array, then the algorithm will go through the whole array of n elements.

  ---

  ### **<div align="center">Complexity</div>**

- **Average | Worst**: We have seen that In the Average | Worst case, the number of elements through which it needs to iterate also depend on **<i>n</i>** which means that Its time complexity in the average and worst case will therefore be **Œò = O(n)**/ **Œò(ùëõ)**:
  - linear complexity =  (will keep on looking until it matches with the given input)


- **Best**: case will be in case the element was found at the first iteration (Was positioned as the first element in the list) thus the time complexity of this algorithm in the best case will be **Œ©(1)** / **Œò(1)**. 

  ---

- **Termination**: The algorithm terminates for every input, either when it finds the element it is looking for, ot when it iterates through the whole array.



<br>
</details>

<details>
<summary>Space Complexity Analysis</summary>
<br>
No auxiliary data structures are required by this algorithm


</br>
</details>


</details>
<br>
<br>


```
 üßÆ Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>

   **c1 * n + c2** |  Where **c1** is the time for one loop iteration, **n** the array length and **c2** the time of the overhead. We now can say that the running time grows with **n**



</details>
<br>
<br>


    
  </details>

  <br>
  
  ---

  

  ### **Binary search**:

 <details >
   <summary>What is Binary search with examples | <b>Click to expand</b></summary>
   </br>

<div align="center">

![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/83/Binary_Search_Depiction.svg/2880px-Binary_Search_Depiction.svg.png)

<small>Image source/ <a href="https://en.wikipedia.org/wiki/Binary_search_algorithm#/media/File:Binary_Search_Depiction.svg">Wikipedia</a></small>
</div>


<br>

**Binary search** is an efficient algorithm for finding an item from a sorted list of items. It works by repeatedly dividing in half the portion of the list that could contain the item, until you've narrowed down the possible locations to just one.

- it only works on sorted lists
- Faster than linear search

<br>

``` 
 ü¶∂üèΩ Steps:
```

<details>
<summary>Table of steps</summary>

<br>

1. Compare target with the middle element.
2. If target matches with middle element, we return the middle index.
3. Else If target is greater than the middle element, then target can only lie in right half subarray after the middle element. So we recur for right half.
4. Else (target is smaller) recur for the left half.


</details>
<br>
<br>


``` 
 üêæ Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>

1. Let left = 0 and right = n-1.

2. If left > right, then stop: target is not present in array. Return -1.
3. Compute middle as the average of right and left, rounded down (so that it is an integer).
4. If array[middle] equals target, then stop. You found it! Return middle.
5. If the middle was too low, that is, array[middle] < target, then set left = middle + 1.
6. Otherwise, the middle was too high. Set max = middle - 1.
7. Go back to step 2.

</details>


<br>
<br>

``` 
 üìü input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>


  Sample input/ Output:
    
``` python
array = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
target = 15

```

  Output : 6 | Element target is present at index 6

  ---

``` python
array[] = {10, 20, 80, 30, 60, 50, 110, 100, 130, 170}
target = 130;    



 # Result: 8

```

</details>
<br>
<br>


```
 üíª Implementation:
```


<details>
<summary>Python üêç</summary>

<br>


  <details>
  <summary>Recursive implementation</summary>

  ```python
def binarySearchHelper(array, target, left, right):

    # Element is not present in the list
    if left > right:
      return - 1
    

    middle = (left + right) // 2
    potentialMatch = array[middle]

    # If element is present at the middle itself
    if target == potentialMatch:
        return middle
    
    # If element is smaller than middle, then it can only
    # be present in left subarray
    elif target < potentialMatch:
        return binarySearchHelper(array, target, left, middle - 1)
        
    # Else the element can only be present in right subarray
    else:
        return binarySearchHelper(array, target, middle + 1, right)

def binarySearch(array, target):
  return binarySearchHelper(array, target, 0, len(array) - 1)



# Test list
array = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
target = 15

# Function call
result = binarySearch(array, target)

if result != -1:
  print("Element is present at index", str(result))
else:
  print("Element is not present in array")


 # Result: 14

  ```
  </details>

  <details>
  <summary>Iterative implementation</summary>
  
  ```python
def binarySearchHelper(array, target, left, right):

    while left <= right:

      middle = (left + right) // 2
      potentialMatch = array[middle]

      # If element is present at the middle itself
      if target == potentialMatch:
          return middle
      
      # If element is smaller than middle, then it can only
      # be present in left subarray
      elif target < potentialMatch:
          right = middle - 1
          
      # Else the element can only be present in right subarray
      else:
          left = middle + 1
    
    # Element is not present in the list
    return -1

def binarySearch(array, target):
  return binarySearchHelper(array, target, 0, len(array) - 1)

# Test list
array = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
target = 15

# Function call
result = binarySearch(array, target)

if result != -1:
  print("Element is present at index", str(result))
else:
  print("Element is not present in array")


 # Result: 14

  ```
</details>





</details>
<br>
<br>


```
 ‚è≥ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>

**Recursive**
Time | Space
----------|----------
O(log(n)) | O(log(n))

<br>
<br>

Iterative

Time | Space
----------|----------
---



</details>
<br>
<br>


```
 üßÆ Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>

The time complexity of Binary Search can be written as

```
T(n) = T(n/2) + c 

```
The above recurrence can be solved either using Recurrence T ree method or Master method. It falls in case II of Master Method and solution of the recurrence is Theta(Logn).

Auxiliary Space: O(1) in case of iterative implementation. In case of recursive implementation, O(Logn) recursion call stack space.



</details>
<br>
<br>
 </details>

   <br>
  
  ---

  

  ### **Jump Search**: 

 <details>
   <summary>What Jump Search with examples | <b>Click to expand</b></summary>
   </br>

``` 
 ü¶∂üèΩ Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 üêæ Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 üìü input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 üíª Implementation:
```


<details>
<summary>Python üêç</summary>

<br>



</details>
<br>
<br>


```
 ‚è≥ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 üßÆ Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br> 
 </details>


   <br>
  
  ---

  

  ### **A<sup>*</sup> Search Algorithm**: 

 <details>
   <summary>What is A* Search Algorithm | <b>Click to expand</b></summary>
   </br>

``` 
 ü¶∂üèΩ Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 üêæ Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 üìü input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 üíª Implementation:
```


<details>
<summary>Python üêç</summary>

<br>



</details>
<br>
<br>


```
 ‚è≥ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 üßÆ Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>

 </details>


   <br>
  
  ---

  

  ### **Ternary search**: 

 <details>
   <summary>What is Ternary search with examples | <b>Click to expand</b></summary>
   </br>

``` 
 ü¶∂üèΩ Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 üêæ Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 üìü input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 üíª Implementation:
```


<details>
<summary>Python üêç</summary>

<br>



</details>
<br>
<br>


```
 ‚è≥ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 üßÆ Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>

 </details>


   <br>
  
  ---

  

  ### **Exponential search**: 

 <details>
   <summary>What is Exponential search with examples | <b>Click to expand</b></summary>
   </br>

``` 
 ü¶∂üèΩ Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 üêæ Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 üìü input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 üíª Implementation:
```


<details>
<summary>Python üêç</summary>

<br>



</details>
<br>
<br>


```
 ‚è≥ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 üßÆ Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>

 </details>

  <br>
<br/>





</div>
</details>

---

<br/>
<br/>
<br/>
<br/>



# 3.5

## **Recursion (Recursive Algorithms)**:
  

<details>
<summary>Recursion cases, explanation & examples</summary>

  <br>

  ### **Differences between Recursion and Iteration**:
  <details>
    <summary> | <b>Click to expand</b></summary>
    </br>

  Property | Recursion | Iteration
  ---------|-----------|------------
  **Definition**|	Function calls itself.|	A set of instructions repeatedly executed.
  **Application**|	For functions.|	For loops.
  **Termination**|	Through base case, where there will be no function call.|	When the termination condition for the iterator ceases to be satisfied.
  **Usage**|	Used when code size needs to be small, and time complexity is not an issue.|	Used when time complexity needs to be balanced against an expanded code size.
  **Code Size** |	Smaller code size|	Larger Code Size.
  **Time Complexity**|	Very high(generally exponential) time complexity.|	Relatively lower time complexity(generally polynomial-logarithmic).|



  </details>

  ---

  <br>

  ### **Base case**:
  <details>
    <summary>Differences between Recursion and Iteration | <b>Click to expand</b></summary>
    </br>
  </details>

  ---

  <br>

  ### **General (recursive) case**:
  <details>
    <summary>Differences between Recursion and Iteration | <b>Click to expand</b></summary>
    </br>

  ---

  </details>




<br/>


</details>

---

<br/>
<br/>
<br/>



# 3.6


## **Sorting Algorithms**:

<details>
<summary>Sorting Algorithms explanation and examples</summary>
<br>

---
  
  ### **Bubble Sort**: 

  <details>
    <summary>What is Bubble Sort with examples | <b>Click to expand</b></summary>
    </br>

``` 
 ü¶∂üèΩ Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 üêæ Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 üìü input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 üíª Implementation:
```


<details>
<summary>Python üêç</summary>

<br>



</details>
<br>
<br>


```
 ‚è≥ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 üßÆ Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>

  </details>

  <br>
  
  ---

  

  ### **Selection Sort**:

 <details>
   <summary>What is Selection Sort with examples<b>Click to expand</b></summary>
   </br>

``` 
 ü¶∂üèΩ Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 üêæ Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 üìü input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 üíª Implementation:
```


<details>
<summary>Python üêç</summary>

<br>



</details>
<br>
<br>


```
 ‚è≥ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 üßÆ Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>

 </details>

   <br>
  
  ---

  

  ### **Insertion Sort**: 

 <details>
   <summary>What is Insertion Sort with examples | <b>Click to expand</b></summary>
   </br>

``` 
 ü¶∂üèΩ Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 üêæ Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 üìü input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 üíª Implementation:
```


<details>
<summary>Python üêç</summary>

<br>



</details>
<br>
<br>


```
 ‚è≥ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 üßÆ Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>

 </details>


   <br>
  
  ---

  

  ### **Merge Sort**: 

 <details>
   <summary>What is Merge Sort with examples | <b>Click to expand</b></summary>
   </br>

``` 
 ü¶∂üèΩ Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 üêæ Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 üìü input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 üíª Implementation:
```


<details>
<summary>Python üêç</summary>

<br>



</details>
<br>
<br>


```
 ‚è≥ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 üßÆ Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>

 </details>


   <br>
  
  ---




  ### **Quick Sort**: 

 <details>
   <summary>What is Quick Sort with examples | <b>Click to expand</b></summary>
   </br>

``` 
 ü¶∂üèΩ Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 üêæ Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 üìü input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 üíª Implementation:
```


<details>
<summary>Python üêç</summary>

<br>



</details>
<br>
<br>


```
 ‚è≥ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 üßÆ Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>

 </details>

   <br>
  
  ---



  ### **Radix Sort**: 

 <details>
   <summary>What is Radix Sort with examples | <b>Click to expand</b></summary>
   </br>

``` 
 ü¶∂üèΩ Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 üêæ Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 üìü input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 üíª Implementation:
```


<details>
<summary>Python üêç</summary>

<br>



</details>
<br>
<br>


```
 ‚è≥ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 üßÆ Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>

 </details>


   <br>
  
  ---



  ### **Counting Sort**: 

 <details>
   <summary>What is Radix Sort with examples | <b>Click to expand</b></summary>
   </br>

``` 
 ü¶∂üèΩ Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 üêæ Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 üìü input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 üíª Implementation:
```


<details>
<summary>Python üêç</summary>

<br>



</details>
<br>
<br>


```
 ‚è≥ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 üßÆ Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>

 </details>

   <br>
  
  ---

  

  ### **Bucket Sort**: 

 <details>
   <summary>What is Bucket Sort with examples | <b>Click to expand</b></summary>
   </br>

``` 
 ü¶∂üèΩ Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 üêæ Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 üìü input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 üíª Implementation:
```


<details>
<summary>Python üêç</summary>

<br>



</details>
<br>
<br>


```
 ‚è≥ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 üßÆ Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>

 </details>

   <br>
</details>

  ---
 
 
  <br>
<br/>
<br/>
<br/>


# 3.7

## **Graph Algorithms**:

  <br>

<details>
<summary>Graph Algorithms explanation and examples</summary>
<br>

---
  


  ### **Breadth-first search**:
  <details>
    <summary>what is Breadth-first search with examples | <b>Click to expand</b></summary>
    </br>

``` 
 ü¶∂üèΩ Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 üêæ Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 üìü input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 üíª Implementation:
```


<details>
<summary>Python üêç</summary>

<br>



</details>
<br>
<br>


```
 ‚è≥ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 üßÆ Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>
  </details>

  ---

  <br>

  ### **Depth-first search**:
  <details>
    <summary>what is Depth-first search with examples | <b>Click to expand</b></summary>
    </br>

``` 
 ü¶∂üèΩ Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 üêæ Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 üìü input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 üíª Implementation:
```


<details>
<summary>Python üêç</summary>

<br>



</details>
<br>
<br>


```
 ‚è≥ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 üßÆ Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>
  </details>

  ---

  <br>

  ### **Dijkstra‚Äôs shortest path algorithm**:
  <details>
    <summary>what is Dijkstra‚Äôs shortest path algorithm with examples | <b>Click to expand</b></summary>
    </br>

``` 
 ü¶∂üèΩ Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 üêæ Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 üìü input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 üíª Implementation:
```


<details>
<summary>Python üêç</summary>

<br>



</details>
<br>
<br>


```
 ‚è≥ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 üßÆ Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>


  </details>


  ---

  <br>

  ### **Bellman‚ÄìFord algorithm**:
  <details>
    <summary>what is Bellman‚ÄìFord algorithm with examples | <b>Click to expand</b></summary>
    </br>

``` 
 ü¶∂üèΩ Steps:
```

<details>
<summary>Table of steps</summary>

<br>



</details>
<br>
<br>


``` 
 üêæ Steps extended:
```

<details>
<summary>Table of steps <b>(Pseudocode)</b></summary>

<br>



</details>


<br>
<br>

``` 
 üìü input/ output:
```

<details>
<summary>Input/ output examples</summary>

<br>



</details>
<br>
<br>


```
 üíª Implementation:
```


<details>
<summary>Python üêç</summary>

<br>



</details>
<br>
<br>


```
 ‚è≥ Time & Space Complexity (in terms of asymptotic notations):
```


<details>
<summary>Asymptotic analysis</summary>

<br>



</details>
<br>
<br>


```
 üßÆ Mathematical Abstraction:
```


<details>
<summary>Algorithm's mathematical explanation</summary>

<br>



</details>
<br>
<br>

  ---

  </details>




<br/>

</details>

---

  <br>
<br/>
<br/>
<br/>



# 3.8

## **Divide and Conquer**:

  <br>




  ### **Breadth-first search**:
  <details>
    <summary> | <b>Click to expand</b></summary>
    </br>

  </details>

  ---

  <br>
<br/>
<br/>
<br/>


# 3.9

## **Dynamic Programming**:

  <br>




  ### **Breadth-first search**:
  <details>
    <summary> | <b>Click to expand</b></summary>
    </br>

  </details>

  ---

  <br>
<br/>
<br/>
<br/>


# 3.10

## **Backtracking**:

  <br>




  ### **Breadth-first search**:
  <details>
    <summary> | <b>Click to expand</b></summary>
    </br>

  </details>

  ---

<br/>
<br/>
<br/>
<br/>







# 4

## **Data structures** 

<br>

<details>
<summary>Data structres | <b>Click to expand</b></summary>

<br>

<div align="center">

![](https://media.giphy.com/media/2UqWA20weXLPRQW5xQ/giphy.gif)

</div>

---

 <h2><b>Data structres</b></h2> 

A data structure is a named location that can be used to store and organize data. And, an algorithm is a collection of steps to solve a particular problem. Learning data structures and algorithms allow us to write efficient and optimized computer programs. 


- Anything that can store data can be called as a data structure, hence Integer, Float, Boolean, Char etc, all are data structures. They are known as Primitive Data Structures.

- We also have some complex Data Structures, which are used to store large and connected data. Some example of Abstract Data Structure are :

  - Linked List
  - Tree
  - Graph
  - Stack, Queue etc.


All these data structures allow us to perform different operations on data. We select these data structures based on which type of operation is required. We will look into these data structures in more details in our later lessons.


Data structures can also be classified on the basis of the following characteristics:

|Characterstic |	Description|
---------------|--------------|
|Linear	| In Linear data structures,the data items are arranged in a linear sequence. Example: Array
|Non-Linear	| In Non-Linear data structures,the data items are not in sequence. Example: Tree, Graph
|Homogeneous	| In homogeneous data structures,all the elements are of same type. Example: Array
|Non-Homogeneous |	In Non-Homogeneous data structure, the elements may or may not be of the same type. Example: Structures
|Static	| Static data structures are those whose sizes and structures associated memory locations are fixed, at compile time. Example: Array
|Dynamic |	Dynamic structures are those which expands or shrinks depending upon the program need and its execution. Also, their associated memory locations changes. Example: Linked List created using pointers


**<small>Cambridge dictionary/ Oxford Languages</small>**

<br>
<details>
<summary>Algorithms Applications & theory | <b>Click to expand üî•</b></summary>

<br>

- **What:** 
  
  ---



- **Applications**: 

___

- **Efficiency**: 

    ---
- **Measuring Efficiency**: 


</details>

</details>

---

<br/>
<br/>
<br/>
<br/>







# 8
## **Articles üì∞**

___

Article           | Provider (Platform) | Used as reference|
--------------------- | -------------- | -------|
[What does ‚ÄòSpace Complexity‚Äô mean?](https://www.geeksforgeeks.org/g-fact-86/) | Geeksforgeeks | Yes
[Data Structures - Algorithms Basics](https://www.tutorialspoint.com/data_structures_algorithms/algorithms_basics.htm) | Tutorialspoint | Yes
[Difference between Big Oh, Big Omega and Big Theta](https://www.tutorialspoint.com/data_structures_algorithms/asymptotic_analysis.htm) | Tutorialspoint | Yes
[Difference between Recursion and Iteration](https://www.geeksforgeeks.org/difference-between-recursion-and-iteration/) | Geeksforgeeks | Yes
[Difference between Big Oh, Big Omega and Big Theta](https://www.geeksforgeeks.org/difference-between-big-oh-big-omega-and-big-theta/) | Geeksforgeeks | Yes

___
<br/><br/><br/>

# 9
## **Books üìö**
One of the most straight to the point Books üî•üïπüìü
___

Book name           | Provider (Platform) | Author| Skill level |  Cost
--------------------- | -------------- | -------- | ---------- | -----
[The Algorithm Design Manual](https://www.amazon.com/-/en/Steven-S-S-Skiena/dp/1849967202) | Amazon | Steven S S. Skiena | Intermediate | $75.98 |
[Grokking Algorithms: An Illustrated Guide for Programmers and Other Curious People](https://www.amazon.com/-/en/Aditya-Bhargava/dp/1617292230) | Amazon | Aditya Bhargava | Beginner | $39.86 |
___

<br/> 

# 10

## **Courses üíª**
----
The most popular courses that teach Algorithms and data structres. Yes for real üî•üïπüìü

Course name           | Provider (Platform) | Duration| Skill level | Course Cost
--------------------- | -------------- | -------- | ---------- | -----
[Intro to Data Structures and Algorithms](https://www.udacity.com/course/data-structures-and-algorithms-in-python--ud513) | [Udacity]([educative.io](https://www.udacity.com/)) | 4 | Beginner | Free |
[Ace the Python Coding Interview](https://www.educative.io/path/ace-python-coding-interview) | [educative](educative.io) | 21h | Beginner |  |

---

<br/><br/><br/>

# 11

<!-- Tables -->
## **Resources**
**Resources** to learn Algorithms and data structres from üê±
<br/>

---
<br/>

> Learn by Watching/ Doing/ Reading
>
Title | Description
------------ | -------------
[Data Structures Crash Course (Algoexpert.io)](https://www.algoexpert.io/data-structures) | The foundational knowledge you need to ace the coding interviews.
[Asymptotic Analysis: Big-O Notation and More](https://www.programiz.com/dsa/asymptotic-notations) | In this tutorial, you will learn what asymptotic notations are. Also, you will learn about Big-O notation, Theta notation and Omega notation.
[What Is Asymptotic Analysis? And Why Does It Matter? A Deeper Understanding of Asymptotic Bounding.](https://www.youtube.com/watch?v=myZKhztFhzE) | What Asymptotic Analysis Is!
[YouTube channel: Back To Back SWE](https://www.youtube.com/channel/UCmJz2DV1a3yfgrR7GqRtUUA) | A very helpful youtube channel to learn and understand Asymptotic Bounding, logratihms, sorting algorithms etc..
[Animated Algorithms and Data Structures by Chris Laux.](https://www.chrislaux.com/) | A interactive website explaining some sorting algorithms and some data structures.
[Recursion in software development](https://livevideo.manning.com/module/31_3_1/algorithms-in-motion/recursion/recursion?) | To understand recursion you must first understand recursion
---
